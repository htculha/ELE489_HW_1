#baÅŸlangÄ±Ã§; data, data frame imize yÃ¼kleniyor. 
from sklearn.datasets import load_wine
import pandas as pd

wine = load_wine(as_frame=True)

# Ã–zellikler ve hedef
X = wine.data
y = wine.target

# BirleÅŸtirilmiÅŸ DataFrame
df = pd.concat([X, y], axis=1)

print(df.head()) 

////

#F parametresi ile feature larÄ±n etkisi kontrol ediliyor
from sklearn.feature_selection import f_classif
import pandas as pd

X = df.drop("target", axis=1)
y = df["target"]

f_values, _ = f_classif(X, y)  # sadece F-value al, p-value almÄ±yoruz

# Sadece F-value iÃ§eren tablo oluÅŸtur
feature_scores = pd.DataFrame({
    'Feature': X.columns,
    'F-value': f_values
}).sort_values(by='F-value', ascending=False)

print(feature_scores)

////

#data lar normalize edilerek frame e aktarÄ±lÄ±yor
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_wine
import pandas as pd

# 1. Veriyi yÃ¼kle
wine = load_wine(as_frame=True)

# 2. Ã–zellikler (X) ve hedef (y) ayÄ±r
X = wine.data
y = wine.target

# 3. Normalize et
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. DataFrame'e Ã§evir (kolon isimleri korunarak)
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# 5. Hedef sÃ¼tunu ekle
df = pd.concat([X_scaled_df, y], axis=1)

# 6. GÃ¶ster
print(df.head())

////

#test ve tranin olarak ikiye ayrÄ±lÄ±yor
from sklearn.model_selection import train_test_split

# Ã–zellikler ve hedefi ayÄ±r
X = df.drop("target", axis=1)
y = df["target"]

# Veriyi ayÄ±r: %80 eÄŸitim, %20 test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0, stratify=y
)
print("EÄŸitim seti boyutu:", X_train.shape)
print("Test seti boyutu:", X_test.shape)

//// 

#k-nn algoritmasÄ± ve uzaklÄ±k modelleri
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 1. Distance fonksiyonlarÄ±
def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))

def manhattan_distance(x1, x2):
    return np.sum(np.abs(x1 - x2))

# 2. k-NN tahmin fonksiyonu
def knn_predict(X_train, y_train, x_test, k=k, distance_func=euclidean_distance):
    distances = []
    for i in range(len(X_train)):
        dist = distance_func(x_test, X_train.iloc[i])
        distances.append((dist, y_train.iloc[i]))
    distances.sort(key=lambda x: x[0])
    k_nearest_labels = [label for (_, label) in distances[:k]]
    most_common = Counter(k_nearest_labels).most_common(1)
    return most_common[0][0]

# 3. K deÄŸerlerini dene
k_values = [1, 3, 5, 7, 9]
euclidean_accuracies = []
manhattan_accuracies = []

for k in k_values:
    # Euclidean
    y_pred_euc = [knn_predict(X_train, y_train, X_test.iloc[i], k=k, distance_func=euclidean_distance)
                  for i in range(len(X_test))]
    acc_euc = accuracy_score(y_test, y_pred_euc)
    euclidean_accuracies.append(acc_euc)

    # Manhattan
    y_pred_man = [knn_predict(X_train, y_train, X_test.iloc[i], k=k, distance_func=manhattan_distance)
                  for i in range(len(X_test))]
    acc_man = accuracy_score(y_test, y_pred_man)
    manhattan_accuracies.append(acc_man)

    print(f"\nðŸ”¹ k = {k}")
    print(f"Euclidean Accuracy: {acc_euc:.2f}")
    print(f"Manhattan Accuracy: {acc_man:.2f}")
    
    print("\nâœ… Confusion Matrix (Euclidean):")
    print(confusion_matrix(y_test, y_pred_euc))
    print(classification_report(y_test, y_pred_euc))

    print("âœ… Confusion Matrix (Manhattan):")
    print(confusion_matrix(y_test, y_pred_man))
    print(classification_report(y_test, y_pred_man))

# 4. GrafiÄŸi Ã§iz
plt.figure(figsize=(9, 5))
plt.plot(k_values, euclidean_accuracies, marker='o', label='Euclidean', color='purple')
plt.plot(k_values, manhattan_accuracies, marker='s', label='Manhattan', color='teal')
plt.title("Accuracy vs K (Euclidean vs Manhattan)")
plt.xlabel("k Value")
plt.ylabel("Accuracy")
plt.xticks(k_values)
plt.grid(True)
plt.legend()





